"""Gradio Web ç•Œé¢ - Book RAG"""
import gradio as gr
from pathlib import Path
from typing import List, Tuple, Optional


def split_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """å°†æ–‡æœ¬åˆ‡åˆ†æˆå°å—"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®åˆ‡åˆ†
        if end < len(text):
            period_pos = text.rfind("ã€‚", start, end)
            exclamation_pos = text.rfind("ï¼", start, end)
            question_pos = text.rfind("ï¼Ÿ", start, end)
            newline_pos = text.rfind("\n", start, end)

            best_pos = max(period_pos, exclamation_pos, question_pos, newline_pos)
            if best_pos > start + chunk_size // 2:
                end = best_pos + 1

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap if end < len(text) else end

    return chunks


# å…¨å±€çŠ¶æ€
class SessionState:
    """ä¼šè¯çŠ¶æ€ç®¡ç†"""
    def __init__(self):
        self.api_key: Optional[str] = None
        self.model: str = "deepseek"
        self.llm_manager = None
        self._vector_store = None
        self._embeddings = None
        self.documents_loaded: bool = False

    @property
    def embeddings(self):
        """å»¶è¿ŸåŠ è½½ embeddings"""
        if self._embeddings is None:
            from src.embeddings import get_embeddings
            self._embeddings = get_embeddings()
        return self._embeddings

    @property
    def vector_store(self):
        """å»¶è¿ŸåŠ è½½å‘é‡å­˜å‚¨"""
        if self._vector_store is None:
            from src.vector_store import get_vector_store
            self._vector_store = get_vector_store()
        return self._vector_store


# æ¨¡å‹é€‰é¡¹
MODEL_OPTIONS = [
    "deepseek",
    "deepseek-reasoner",
    "gpt-4",
    "gpt-3.5",
    "claude-opus",
    "claude-sonnet",
    "gemini",
    "llama",
]


def process_upload(files: List, state: SessionState, progress: gr.Progress = gr.Progress()) -> str:
    """å¤„ç†æ–‡ä»¶ä¸Šä¼  - æ”¯æŒæµå¼è¿›åº¦æ˜¾ç¤º"""
    if not files:
        return "âŒ è¯·é€‰æ‹©æ–‡ä»¶"

    count = 0
    total_chunks = 0
    status_lines = []

    from src.loaders import get_loader
    from src.loaders.base import Document

    total_steps = len(files) * 3  # æ¯ä¸ªæ–‡ä»¶3æ­¥ï¼šè§£æã€åˆ‡åˆ†ã€embedding
    current_step = 0

    for file_idx, file in enumerate(files, 1):
        file_path = file.name
        path = Path(file_path)

        try:
            # æ­¥éª¤1: è§£ææ–‡æ¡£
            current_step += 1
            progress(current_step / total_steps, desc=f"ğŸ“– [{file_idx}/{len(files)}] æ­£åœ¨è§£æ {path.name}...")

            loader = get_loader(str(path))
            documents = loader.load(str(path))

            status_lines.append(f"ğŸ“– [{file_idx}/{len(files)}] {path.name}: å·²æå– {len(documents)} é¡µ")

            # æ­¥éª¤2: åˆ‡åˆ†æ–‡æ¡£
            current_step += 1
            progress(current_step / total_steps, desc=f"âœ‚ï¸ [{file_idx}/{len(files)}] æ­£åœ¨åˆ‡åˆ† {path.name}...")

            chunked_docs = []
            for doc in documents:
                chunks = split_text(doc.content)
                for i, chunk in enumerate(chunks):
                    chunked_doc = Document(
                        content=chunk,
                        metadata={
                            **doc.metadata,
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                        },
                        source=doc.source,
                    )
                    chunked_docs.append(chunked_doc)

            status_lines.append(f"âœ‚ï¸ [{file_idx}/{len(files)}] {path.name}: å·²åˆ‡åˆ† {len(chunked_docs)} å—")

            # æ¸…é™¤æ—§æ•°æ®
            state.vector_store.delete_by_source(str(path))

            # æ­¥éª¤3: ç”Ÿæˆ embeddings
            current_step += 1
            progress(current_step / total_steps, desc=f"ğŸ”¢ [{file_idx}/{len(files)}] æ­£åœ¨ç”Ÿæˆ embeddings ({len(chunked_docs)} å—)...")

            state.vector_store.add_documents(chunked_docs)

            status_lines.append(f"âœ… [{file_idx}/{len(files)}] {path.name}: å®Œæˆï¼å…± {len(chunked_docs)} ä¸ªå—")
            count += 1
            total_chunks += len(chunked_docs)

        except Exception as e:
            status_lines.append(f"âŒ [{file_idx}/{len(files)}] {path.name}: {str(e)}")

    state.documents_loaded = count > 0

    summary = f"ğŸ“Š å·²ä¸Šä¼  {count} ä¸ªæ–‡ä»¶ï¼Œå…± {total_chunks} ä¸ªæ–‡æ¡£å—\n\n" + "\n".join(status_lines)
    return summary


def chat_response(
    message: str,
    history: List[dict],
    api_key: str,
    model: str,
    state: SessionState,
) -> List[dict]:
    """å¤„ç†é—®ç­” - è¿”å› messages æ ¼å¼"""
    if not message.strip():
        return history

    # æ£€æŸ¥ API Key
    if not api_key:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆé…ç½® OpenRouter API Key"})
        return history

    # æ›´æ–° LLM ç®¡ç†å™¨
    if api_key != state.api_key or model != state.model:
        from src.chains.llm_manager import LLMManager
        state.api_key = api_key
        state.model = model
        state.llm_manager = LLMManager(api_key=api_key, default_model=model)
    elif state.llm_manager is None:
        from src.chains.llm_manager import LLMManager
        state.llm_manager = LLMManager(api_key=api_key, default_model=model)

    # æ£€æŸ¥æ–‡æ¡£
    if not state.documents_loaded:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£"})
        return history

    try:
        from src.chains.qa_chain import QAChain
        from src.retriever.base import Retriever

        # åˆ›å»º QA é“¾
        retriever = Retriever(vector_store=state.vector_store)
        qa_chain = QAChain(retriever=retriever, llm_manager=state.llm_manager)

        # æ‰§è¡Œé—®ç­”
        result = qa_chain.run(message)

        # æ ¼å¼åŒ–å“åº”
        response = result.answer

        # æ·»åŠ å¼•ç”¨
        if result.citations:
            response += "\n\n---\n**ğŸ“š æ¥æºå¼•ç”¨:**\n"
            for citation in result.citations:
                response += f"\nğŸ“– ã€Š{citation.book_title}ã€‹{citation.chapter_title} (ç¬¬{citation.page_num}é¡µ)\n"

        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})

    except Exception as e:
        error_msg = f"âŒ é—®ç­”å‡ºé”™: {str(e)}"
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})

    return history


def create_interface() -> gr.Blocks:
    """åˆ›å»º Gradio ç•Œé¢"""

    state = SessionState()

    with gr.Blocks(
        title="Book RAG - çŸ¥è¯†åº“é—®ç­”",
        analytics_enabled=False,
    ) as app:

        gr.Markdown(
            """
            # ğŸ“š Book RAG - çŸ¥è¯†åº“é—®ç­”

            ä¸Šä¼ æ–‡æ¡£ï¼Œé…ç½® API Keyï¼Œå¼€å§‹æ™ºèƒ½é—®ç­”ï¼æ”¯æŒ PDFã€DOCXã€MDã€EPUB æ ¼å¼ã€‚
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ é…ç½®")

                api_key_input = gr.Textbox(
                    label="OpenRouter API Key",
                    placeholder="è¾“å…¥ä½ çš„ OpenRouter API Key (sk-or-v1...)",
                    type="password",
                    value="",
                )

                model_dropdown = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=MODEL_OPTIONS,
                    value="deepseek",
                )

                gr.Markdown("""
                **ä½¿ç”¨è¯´æ˜:**
                1. è¾“å…¥ OpenRouter API Key
                2. é€‰æ‹© LLM æ¨¡å‹
                3. ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶
                4. å¼€å§‹é—®ç­”
                """)

            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“„ æ–‡æ¡£ä¸Šä¼ ")

                file_upload = gr.File(
                    label="ä¸Šä¼ æ–‡æ¡£",
                    file_count="multiple",
                    file_types=[".pdf", ".docx", ".doc", ".md", ".markdown", ".epub"],
                )

                upload_status = gr.Textbox(
                    label="ä¸Šä¼ çŠ¶æ€",
                    lines=5,
                    interactive=False,
                    value="ç­‰å¾…ä¸Šä¼ ...",
                )

                upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£", variant="primary", size="lg")

        gr.Markdown("### ğŸ’¬ é—®ç­”")

        chatbot = gr.Chatbot(
            label="å¯¹è¯å†å²",
            height=400,
        )

        with gr.Row():
            chat_input = gr.Textbox(
                label="è¾“å…¥é—®é¢˜",
                placeholder="è¾“å…¥ä½ çš„é—®é¢˜...",
                scale=4,
                autofocus=True,
            )
            submit_btn = gr.Button("å‘é€", variant="primary", scale=1, size="lg")

        with gr.Row():
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")

        # ç¤ºä¾‹é—®é¢˜
        gr.Examples(
            examples=[
                "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "æ€»ç»“ä¸€ä¸‹æ ¸å¿ƒè§‚ç‚¹",
                "æœ‰ä»€ä¹ˆå…³é”®ç»“è®ºï¼Ÿ",
            ],
            inputs=chat_input,
            label="ç¤ºä¾‹é—®é¢˜",
        )

        # äº‹ä»¶ç»‘å®š
        def handle_upload(files):
            return process_upload(files, state)

        upload_btn.click(
            fn=handle_upload,
            inputs=[file_upload],
            outputs=[upload_status],
        )

        def handle_chat(message, history, api_key, model):
            return chat_response(message, history, api_key, model, state)

        submit_btn.click(
            fn=handle_chat,
            inputs=[chat_input, chatbot, api_key_input, model_dropdown],
            outputs=[chatbot],
        ).then(
            lambda: "",
            outputs=[chat_input],
        )

        chat_input.submit(
            fn=handle_chat,
            inputs=[chat_input, chatbot, api_key_input, model_dropdown],
            outputs=[chatbot],
        ).then(
            lambda: "",
            outputs=[chat_input],
        )

        clear_btn.click(
            fn=lambda: [],
            outputs=[chatbot],
        )

    return app


if __name__ == "__main__":
    import sys

    print("ğŸš€ Starting Gradio app...", file=sys.stderr, flush=True)
    print("ğŸ“¦ Loading modules...", file=sys.stderr, flush=True)

    app = create_interface()

    print("ğŸ“± Interface created, launching...", file=sys.stderr, flush=True)
    print("ğŸŒ Open http://127.0.0.1:7861 in your browser", file=sys.stderr, flush=True)

    app.launch(
        server_name="127.0.0.1",
        server_port=7861,
        share=False,
        show_error=True,
        quiet=False,
    )
