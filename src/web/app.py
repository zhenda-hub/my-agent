"""Gradio Web ç•Œé¢ - Book RAG"""
import gradio as gr
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any


def split_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """å°†æ–‡æœ¬åˆ‡åˆ†æˆå°å—"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®åˆ‡åˆ†
        if end < len(text):
            period_pos = text.rfind("ã€‚", start, end)
            exclamation_pos = text.rfind("ï¼", start, end)
            question_pos = text.rfind("ï¼Ÿ", start, end)
            newline_pos = text.rfind("\n", start, end)

            best_pos = max(period_pos, exclamation_pos, question_pos, newline_pos)
            if best_pos > start + chunk_size // 2:
                end = best_pos + 1

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap if end < len(text) else end

    return chunks


# å…¨å±€çŠ¶æ€
class SessionState:
    """ä¼šè¯çŠ¶æ€ç®¡ç†"""
    def __init__(self):
        self.api_key: Optional[str] = None
        self.model: str = "deepseek"
        self.llm_manager = None
        self._vector_store = None
        self._embeddings = None
        self.documents_loaded: bool = False
        self.current_citations: List = []  # å½“å‰é—®ç­”çš„å¼•ç”¨åˆ—è¡¨
        self.selected_sources: List[str] = []  # ç”¨æˆ·é€‰ä¸­çš„æ–‡ä»¶æ¥æº

    @property
    def embeddings(self):
        """å»¶è¿ŸåŠ è½½ embeddings"""
        if self._embeddings is None:
            from src.embeddings import get_embeddings
            self._embeddings = get_embeddings()
        return self._embeddings

    @property
    def vector_store(self):
        """å»¶è¿ŸåŠ è½½å‘é‡å­˜å‚¨"""
        if self._vector_store is None:
            from src.vector_store import get_vector_store
            self._vector_store = get_vector_store()
        return self._vector_store


def get_initial_models() -> list:
    """
    è·å–åˆå§‹æ¨¡å‹åˆ—è¡¨ï¼ˆä» OpenRouter API åŠ¨æ€è·å–å…è´¹æ¨¡å‹ï¼‰

    Returns:
        æ¨¡å‹ ID åˆ—è¡¨
    """
    import os
    from dotenv import load_dotenv

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    try:
        api_key = os.getenv("OPENROUTER_API_KEY", "")
        if api_key:
            from src.chains.llm_manager import LLMManager
            llm = LLMManager(api_key=api_key)
            models = llm.get_free_models()
            if models:
                return models
    except Exception as e:
        print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}", file=sys.stderr)

    # é™çº§åˆ°é»˜è®¤æ¨¡å‹
    return ["deepseek"]


def process_upload(files: List, state: SessionState, progress: gr.Progress = gr.Progress()) -> str:
    """å¤„ç†æ–‡ä»¶ä¸Šä¼  - æ”¯æŒæµå¼è¿›åº¦æ˜¾ç¤ºï¼Œè·³è¿‡å·²ä¸Šä¼ çš„æ–‡ä»¶"""
    if not files:
        return "âŒ è¯·é€‰æ‹©æ–‡ä»¶"

    count = 0
    skipped = 0
    total_chunks = 0
    status_lines = []

    from src.loaders import get_loader
    from src.loaders.base import Document

    # ç»Ÿè®¡éœ€è¦å¤„ç†çš„æ–‡ä»¶æ•°é‡
    files_to_process = []
    skipped_files = []

    for file in files:
        file_path = file.name
        path = Path(file_path)
        if state.vector_store.source_exists(str(path)):
            skipped_files.append(path.name)
        else:
            files_to_process.append(file)

    total_steps = len(files_to_process) * 3  # æ¯ä¸ªæ–‡ä»¶3æ­¥ï¼šè§£æã€åˆ‡åˆ†ã€embedding
    current_step = 0

    # å…ˆæŠ¥å‘Šè·³è¿‡çš„æ–‡ä»¶
    if skipped_files:
        status_lines.append(f"â­ï¸ è·³è¿‡ {len(skipped_files)} ä¸ªå·²ä¸Šä¼ çš„æ–‡ä»¶: {', '.join(skipped_files)}")
        skipped = len(skipped_files)

    for file_idx, file in enumerate(files_to_process, 1):
        file_path = file.name
        path = Path(file_path)

        try:
            # æ­¥éª¤1: è§£ææ–‡æ¡£
            current_step += 1
            progress(current_step / total_steps, desc=f"ğŸ“– [{file_idx}/{len(files_to_process)}] æ­£åœ¨è§£æ {path.name}...")

            loader = get_loader(str(path))
            documents = loader.load(str(path))

            status_lines.append(f"ğŸ“– [{file_idx}/{len(files_to_process)}] {path.name}: å·²æå– {len(documents)} é¡µ")

            # æ­¥éª¤2: åˆ‡åˆ†æ–‡æ¡£
            current_step += 1
            progress(current_step / total_steps, desc=f"âœ‚ï¸ [{file_idx}/{len(files_to_process)}] æ­£åœ¨åˆ‡åˆ† {path.name}...")

            chunked_docs = []
            for doc in documents:
                chunks = split_text(doc.content)
                for i, chunk in enumerate(chunks):
                    chunked_doc = Document(
                        content=chunk,
                        metadata={
                            **doc.metadata,
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                        },
                        source=doc.source,
                    )
                    chunked_docs.append(chunked_doc)

            status_lines.append(f"âœ‚ï¸ [{file_idx}/{len(files_to_process)}] {path.name}: å·²åˆ‡åˆ† {len(chunked_docs)} å—")

            # æ¸…é™¤æ—§æ•°æ®
            state.vector_store.delete_by_source(str(path))

            # æ­¥éª¤3: ç”Ÿæˆ embeddings
            current_step += 1
            progress(current_step / total_steps, desc=f"ğŸ”¢ [{file_idx}/{len(files_to_process)}] æ­£åœ¨ç”Ÿæˆ embeddings ({len(chunked_docs)} å—)...")

            state.vector_store.add_documents(chunked_docs)

            status_lines.append(f"âœ… [{file_idx}/{len(files_to_process)}] {path.name}: å®Œæˆï¼å…± {len(chunked_docs)} ä¸ªå—")
            count += 1
            total_chunks += len(chunked_docs)

        except Exception as e:
            status_lines.append(f"âŒ [{file_idx}/{len(files_to_process)}] {path.name}: {str(e)}")

    state.documents_loaded = count > 0 or skipped > 0

    total_processed = count + skipped
    summary = f"ğŸ“Š å…± {total_processed} ä¸ªæ–‡ä»¶ (æ–°å¢ {count} ä¸ªï¼Œè·³è¿‡ {skipped} ä¸ªå·²å­˜åœ¨)ï¼Œå…± {total_chunks} ä¸ªæ–‡æ¡£å—\n\n" + "\n".join(status_lines)
    return summary


def process_url(url: str, state: SessionState) -> str:
    """å¤„ç† URL æŠ“å–"""
    if not url or not url.strip():
        return "âŒ è¯·è¾“å…¥ URL"

    url = url.strip()

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if state.vector_store.source_exists(url):
        return f"â­ï¸ URL å·²å­˜åœ¨: {url}"

    try:
        from src.loaders.web_loader import WebLoader
        from src.loaders.base import Document

        # æŠ“å–ç½‘é¡µ
        loader = WebLoader()
        documents = loader.load(url)

        # åˆ‡åˆ†æ–‡æ¡£
        chunked_docs = []
        for doc in documents:
            chunks = split_text(doc.content)
            for i, chunk in enumerate(chunks):
                chunked_doc = Document(
                    content=chunk,
                    metadata={
                        **doc.metadata,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                    },
                    source=doc.source,
                )
                chunked_docs.append(chunked_doc)

        # å­˜å‚¨åˆ°å‘é‡åº“
        state.vector_store.add_documents(chunked_docs)
        state.documents_loaded = True

        return f"âœ… æˆåŠŸæŠ“å–: {url}\nğŸ“Š å…± {len(chunked_docs)} ä¸ªæ–‡æ¡£å—"

    except Exception as e:
        return f"âŒ æŠ“å–å¤±è´¥: {url}\né”™è¯¯: {str(e)}"


def refresh_file_list(state: SessionState) -> Tuple[List[str], str]:
    """
    åˆ·æ–°æ–‡ä»¶åˆ—è¡¨ - ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶

    Returns:
        (æ–‡ä»¶ååˆ—è¡¨, ä¿¡æ¯æ–‡æœ¬)
    """
    all_sources = state.vector_store.get_all_sources()

    # æå–æ–‡ä»¶åï¼ˆä»å®Œæ•´è·¯å¾„ï¼‰
    filenames = [Path(src).name for src in all_sources]

    # é»˜è®¤å…¨é€‰
    state.selected_sources = all_sources

    info = f"ğŸ“ æ•°æ®åº“ä¸­å…±æœ‰ {len(all_sources)} ä¸ªæ–‡ä»¶"
    return filenames, info


def update_selected_sources(selected_filenames: List[str], state: SessionState) -> None:
    """
    æ›´æ–°ç”¨æˆ·é€‰ä¸­çš„æ–‡ä»¶

    Args:
        selected_filenames: ç”¨æˆ·åœ¨ç•Œé¢ä¸Šé€‰ä¸­çš„æ–‡ä»¶ååˆ—è¡¨
        state: ä¼šè¯çŠ¶æ€
    """
    all_sources = state.vector_store.get_all_sources()

    # å°†æ–‡ä»¶åæ˜ å°„å›å®Œæ•´è·¯å¾„
    filename_to_source = {Path(src).name: src for src in all_sources}

    # æ›´æ–°é€‰ä¸­çš„ sources
    state.selected_sources = [
        filename_to_source[name] for name in selected_filenames
        if name in filename_to_source
    ]


def chat_response(
    message: str,
    history: List[dict],
    api_key: str,
    model: str,
    state: SessionState,
) -> List[dict]:
    """å¤„ç†é—®ç­” - è¿”å› messages æ ¼å¼"""
    if not message.strip():
        return history

    # æ£€æŸ¥ API Key
    if not api_key:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆé…ç½® OpenRouter API Key"})
        return history

    # æ›´æ–° LLM ç®¡ç†å™¨
    if api_key != state.api_key or model != state.model:
        from src.chains.llm_manager import LLMManager
        state.api_key = api_key
        state.model = model
        state.llm_manager = LLMManager(api_key=api_key, default_model=model)
    elif state.llm_manager is None:
        from src.chains.llm_manager import LLMManager
        state.llm_manager = LLMManager(api_key=api_key, default_model=model)

    # æ£€æŸ¥æ–‡æ¡£
    if not state.documents_loaded:
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£"})
        return history

    try:
        from src.chains.qa_chain import QAChain
        from src.retriever.base import Retriever

        # åˆ›å»ºæ£€ç´¢å™¨æ—¶æ·»åŠ  source è¿‡æ»¤
        filter_dict = None
        if state.selected_sources:
            filter_dict = {"source": {"$in": state.selected_sources}}

        retriever = Retriever(
            vector_store=state.vector_store,
            filter_metadata=filter_dict
        )
        qa_chain = QAChain(retriever=retriever, llm_manager=state.llm_manager)

        # æ‰§è¡Œé—®ç­”
        result = qa_chain.run(message)

        # ä¿å­˜å¼•ç”¨åˆ°çŠ¶æ€ä¸­
        state.current_citations = result.citations

        # ç›´æ¥ä½¿ç”¨æ ¼å¼åŒ–åçš„ç­”æ¡ˆï¼ˆåŒ…å«å¼•ç”¨å†…å®¹ï¼‰
        response = result.answer

        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})

    except Exception as e:
        error_msg = f"âŒ é—®ç­”å‡ºé”™: {str(e)}"
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})

    return history


def create_interface() -> gr.Blocks:
    """åˆ›å»º Gradio ç•Œé¢"""
    state = SessionState()

    # å¯åŠ¨æ—¶è·å–å…è´¹æ¨¡å‹åˆ—è¡¨
    initial_models = get_initial_models()

    with gr.Blocks(
        title="Book RAG - çŸ¥è¯†åº“é—®ç­”",
        analytics_enabled=False,
    ) as app:

        gr.Markdown(
            """
            # ğŸ“š Book RAG - çŸ¥è¯†åº“é—®ç­”

            ä¸Šä¼ æ–‡æ¡£ï¼Œé…ç½® API Keyï¼Œå¼€å§‹æ™ºèƒ½é—®ç­”ï¼æ”¯æŒ PDFã€DOCXã€TXTã€MDã€EPUB æ ¼å¼ã€‚
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ é…ç½®")

                api_key_input = gr.Textbox(
                    label="OpenRouter API Key",
                    placeholder="è¾“å…¥ä½ çš„ OpenRouter API Key (sk-or-v1...)",
                    type="password",
                    value="",
                )

                model_dropdown = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=initial_models,
                    value=initial_models[0] if initial_models else "deepseek",
                )

                gr.Markdown("""
                **ä½¿ç”¨è¯´æ˜:**
                1. è¾“å…¥ OpenRouter API Key
                2. é€‰æ‹© LLM æ¨¡å‹
                3. ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶
                4. å¼€å§‹é—®ç­”
                """)

            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“„ æ–‡æ¡£ä¸Šä¼ ")

                file_upload = gr.File(
                    label="ä¸Šä¼ æ–‡æ¡£",
                    file_count="multiple",
                    file_types=[".pdf", ".docx", ".doc", ".txt", ".md", ".markdown", ".epub"],
                )

                upload_status = gr.Textbox(
                    label="ä¸Šä¼ çŠ¶æ€",
                    lines=5,
                    interactive=False,
                    value="ç­‰å¾…ä¸Šä¼ ...",
                )

                upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£", variant="primary", size="lg")

                gr.Markdown("---")

                gr.Markdown("### ğŸŒ ç½‘é¡µæŠ“å–")

                url_input = gr.Textbox(
                    label="ç½‘é¡µ URL",
                    placeholder="è¾“å…¥ç½‘å€ï¼Œå¦‚ https://example.com",
                )

                url_status = gr.Textbox(
                    label="æŠ“å–çŠ¶æ€",
                    lines=3,
                    interactive=False,
                    value="ç­‰å¾…è¾“å…¥...",
                )

                url_btn = gr.Button("ğŸ”— æŠ“å–ç½‘é¡µ", variant="secondary", size="lg")

        gr.Markdown("### ğŸ“ å·²ä¸Šä¼ æ–‡ä»¶")

        with gr.Row():
            file_list_info = gr.Textbox(
                label="æ–‡ä»¶åˆ—è¡¨",
                value="ç‚¹å‡»åˆ·æ–°æŸ¥çœ‹æ–‡ä»¶...",
                interactive=False,
                scale=3
            )
            refresh_files_btn = gr.Button("ğŸ”„ åˆ·æ–°", scale=1)

        file_checkbox = gr.CheckboxGroup(
            label="é€‰æ‹©ç”¨äº RAG çš„æ–‡ä»¶ï¼ˆæœªé€‰æ‹©=ä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ï¼‰",
            choices=[],
            value=[],
        )

        gr.Markdown("### ğŸ’¬ é—®ç­”")

        chatbot = gr.Chatbot(
            label="å¯¹è¯å†å²",
            height=400,
            sanitize_html=False,  # å…è®¸ HTML æ ‡ç­¾ï¼ˆç”¨äºå¯æŠ˜å å¼•ç”¨ï¼‰
        )

        with gr.Row():
            chat_input = gr.Textbox(
                label="è¾“å…¥é—®é¢˜",
                placeholder="è¾“å…¥ä½ çš„é—®é¢˜...",
                scale=4,
                autofocus=True,
            )
            submit_btn = gr.Button("å‘é€", variant="primary", scale=1, size="lg")

        with gr.Row():
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")

        # ç¤ºä¾‹é—®é¢˜
        gr.Examples(
            examples=[
                "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "æ€»ç»“ä¸€ä¸‹æ ¸å¿ƒè§‚ç‚¹",
                "æœ‰ä»€ä¹ˆå…³é”®ç»“è®ºï¼Ÿ",
            ],
            inputs=chat_input,
            label="ç¤ºä¾‹é—®é¢˜",
        )

        # äº‹ä»¶ç»‘å®š
        # å®šä¹‰å¤„ç†å‡½æ•°
        def handle_upload(files):
            return process_upload(files, state)

        def handle_url(url):
            return process_url(url, state)

        # åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
        def handle_refresh():
            filenames, info = refresh_file_list(state)
            return filenames, info, filenames

        # æ›´æ–°é€‰ä¸­æ–‡ä»¶
        def handle_file_selection(selected_filenames):
            update_selected_sources(selected_filenames, state)
            return None

        # ä¸Šä¼ åè‡ªåŠ¨åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
        upload_btn.click(
            fn=handle_upload,
            inputs=[file_upload],
            outputs=[upload_status],
        ).then(
            fn=handle_refresh,
            inputs=[],
            outputs=[file_checkbox, file_list_info, file_checkbox],
        )

        # URL æŠ“å–åè‡ªåŠ¨åˆ·æ–°æ–‡ä»¶åˆ—è¡¨
        url_btn.click(
            fn=handle_url,
            inputs=[url_input],
            outputs=[url_status],
        ).then(
            fn=handle_refresh,
            inputs=[],
            outputs=[file_checkbox, file_list_info, file_checkbox],
        ).then(
            lambda: "",
            outputs=[url_input],
        )

        # åˆ·æ–°æ–‡ä»¶åˆ—è¡¨æŒ‰é’®
        refresh_files_btn.click(
            fn=handle_refresh,
            inputs=[],
            outputs=[file_checkbox, file_list_info, file_checkbox],
        )

        # æ–‡ä»¶é€‰æ‹©å˜åŒ–
        file_checkbox.change(
            fn=handle_file_selection,
            inputs=[file_checkbox],
            outputs=[],
        )

        def handle_chat(message, history, api_key, model):
            return chat_response(message, history, api_key, model, state)

        submit_btn.click(
            fn=handle_chat,
            inputs=[chat_input, chatbot, api_key_input, model_dropdown],
            outputs=[chatbot],
        ).then(
            lambda: "",
            outputs=[chat_input],
        )

        chat_input.submit(
            fn=handle_chat,
            inputs=[chat_input, chatbot, api_key_input, model_dropdown],
            outputs=[chatbot],
        ).then(
            lambda: "",
            outputs=[chat_input],
        )

        clear_btn.click(
            fn=lambda: [],
            outputs=[chatbot],
        )

    return app


if __name__ == "__main__":
    import sys

    print("ğŸš€ Starting Gradio app...", file=sys.stderr, flush=True)
    print("ğŸ“¦ Loading modules...", file=sys.stderr, flush=True)

    app = create_interface()

    print("ğŸ“± Interface created, launching...", file=sys.stderr, flush=True)
    print("ğŸŒ Open http://127.0.0.1:7861 in your browser", file=sys.stderr, flush=True)

    app.launch(
        server_name="127.0.0.1",
        server_port=7861,
        share=False,
        show_error=True,
        quiet=False,
    )
