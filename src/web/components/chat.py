"""èŠå¤©ç•Œé¢ç»„ä»¶"""
import streamlit as st
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from src.vector_store import VectorStore


def generate_response(prompt: str, vector_store: "VectorStore") -> dict:
    """ç”Ÿæˆå›å¤

    Args:
        prompt: ç”¨æˆ·é—®é¢˜
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹

    Returns:
        {"answer": str, "citations": list} æ ¼å¼çš„å›å¤
    """
    # éªŒè¯ API Key
    if not st.session_state.api_key:
        return {"answer": "âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key", "citations": []}

    # éªŒè¯æ–‡æ¡£
    if not st.session_state.documents_loaded:
        return {"answer": "âš ï¸ è¯·å…ˆä¸Šä¼ æ–‡æ¡£", "citations": []}

    # æ›´æ–° LLM ç®¡ç†å™¨
    from src.chains.llm_manager import LLMManager
    if st.session_state.llm_manager is None:
        st.session_state.llm_manager = LLMManager(
            api_key=st.session_state.api_key,
            default_model=st.session_state.selected_model
        )

    try:
        from src.chains.qa_chain import QAChain
        from src.retriever.base import Retriever

        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆå¸¦è¿‡æ»¤ï¼‰
        filter_dict = None
        if st.session_state.selected_sources:
            filter_dict = {"source": {"$in": st.session_state.selected_sources}}

        retriever = Retriever(vector_store=vector_store, filter_metadata=filter_dict)
        qa_chain = QAChain(retriever=retriever, llm_manager=st.session_state.llm_manager)

        # æ‰§è¡Œé—®ç­”
        result = qa_chain.run(prompt)

        return {
            "answer": result.answer,
            "citations": result.citations
        }

    except Exception as e:
        return {
            "answer": f"âŒ å‡ºé”™äº†ï¼š{e}",
            "citations": []
        }


def render_chat_interface(vector_store: "VectorStore") -> None:
    """æ¸²æŸ“èŠå¤©ç•Œé¢

    Args:
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
    """
    st.header("ğŸ’¬ é—®ç­”")

    # æ˜¾ç¤ºèŠå¤©å†å²
    chat_container = st.container()
    with chat_container:
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                if msg.get("citations"):
                    with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨"):
                        for citation in msg["citations"]:
                            st.caption(f"- {citation}")

    # æ¸…ç©ºå¯¹è¯æŒ‰é’®ï¼ˆåœ¨èŠå¤©å†å²ä¸‹æ–¹ï¼‰
    if st.session_state.chat_history:
        if st.button("æ¸…ç©ºå¯¹è¯", key="clear_chat"):
            st.session_state.chat_history = []
            st.rerun()

    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.chat_history.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”ŸæˆåŠ©æ‰‹å›å¤
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                response = generate_response(prompt, vector_store)
                st.markdown(response["answer"])

                if response.get("citations"):
                    with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨"):
                        for citation in response["citations"]:
                            st.caption(f"- {citation}")

                # æ·»åŠ åˆ°å†å²
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": response["answer"],
                    "citations": response.get("citations", [])
                })

        st.rerun()
