"""æ–‡æ¡£ç®¡ç†ç»„ä»¶"""
import streamlit as st
import tempfile
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from src.vector_store import VectorStore


def split_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:
    """å°†æ–‡æœ¬åˆ‡åˆ†æˆå°å—"""
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®åˆ‡åˆ†
        if end < len(text):
            period_pos = text.rfind("ã€‚", start, end)
            exclamation_pos = text.rfind("ï¼", start, end)
            question_pos = text.rfind("ï¼Ÿ", start, end)
            newline_pos = text.rfind("\n", start, end)

            best_pos = max(period_pos, exclamation_pos, question_pos, newline_pos)
            if best_pos > start + chunk_size // 2:
                end = best_pos + 1

        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        start = end - overlap if end < len(text) else end

    return chunks


def render_document_panel(vector_store: "VectorStore") -> None:
    """æ¸²æŸ“æ–‡æ¡£ä¸Šä¼ é¢æ¿

    Args:
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
    """
    with st.expander("ğŸ“„ ä¸Šä¼ æ–‡æ¡£", expanded=True):
        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            accept_multiple_files=True,
            type=['pdf', 'docx', 'txt', 'md', 'epub'],
            help="æ”¯æŒï¼šPDF, DOCX, TXT, MD, EPUB"
        )

        if uploaded_files and st.button("ä¸Šä¼ ", type="primary", use_container_width=True):
            with st.status("æ­£åœ¨å¤„ç†...", expanded=True) as status:
                from src.loaders import get_loader
                from src.loaders.base import Document

                total = len(uploaded_files)
                for i, file in enumerate(uploaded_files):
                    status.update(label=f"å¤„ç† {file.name} ({i+1}/{total})")

                    # ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸º sourceï¼ˆåŠ ä¸Šå‰ç¼€é¿å…å†²çªï¼‰
                    original_source = f"upload:{file.name}"

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if vector_store.source_exists(original_source):
                        st.info(f"â­ï¸ {file.name} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        continue

                    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.name).suffix) as f:
                        f.write(file.getvalue())
                        temp_path = f.name

                    try:
                        path = Path(temp_path)

                        # åŠ è½½æ–‡æ¡£
                        loader = get_loader(str(path))
                        documents = loader.load(str(path))

                        # åˆ‡åˆ†æ–‡æ¡£
                        chunked_docs = []
                        for doc in documents:
                            chunks = split_text(doc.content)
                            for j, chunk in enumerate(chunks):
                                chunked_doc = Document(
                                    content=chunk,
                                    metadata={
                                        **doc.metadata,
                                        "chunk_index": j,
                                        "total_chunks": len(chunks),
                                        "original_filename": file.name,  # ä¿å­˜åŸå§‹æ–‡ä»¶å
                                    },
                                    source=original_source,  # ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸º source
                                )
                                chunked_docs.append(chunked_doc)

                        # æ¸…é™¤æ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        vector_store.delete_by_source(original_source)

                        # å­˜å‚¨åˆ°å‘é‡åº“
                        vector_store.add_documents(chunked_docs)
                        st.success(f"âœ… {file.name}: {len(chunked_docs)} ä¸ªå—")

                    except Exception as e:
                        st.error(f"âŒ {file.name}: {e}")
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            Path(temp_path).unlink(missing_ok=True)
                        except:
                            pass

                status.update(label="å®Œæˆï¼", state="complete")
                st.session_state.documents_loaded = True
                st.rerun()


def render_web_scraping(vector_store: "VectorStore") -> None:
    """æ¸²æŸ“ç½‘é¡µæŠ“å–é¢æ¿

    Args:
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
    """
    with st.expander("ğŸ”— ç½‘é¡µæŠ“å–"):
        url = st.text_input("ç½‘é¡µ URL", placeholder="https://example.com", key="web_url")

        if st.button("æŠ“å–", use_container_width=True, key="scrape_btn"):
            if url and url.strip():
                url = url.strip()

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if vector_store.source_exists(url):
                    st.warning(f"â­ï¸ URL å·²å­˜åœ¨: {url}")
                    return

                with st.spinner("æ­£åœ¨æŠ“å–..."):
                    try:
                        from src.loaders.web_loader import WebLoader
                        from src.loaders.base import Document

                        # æŠ“å–ç½‘é¡µ
                        loader = WebLoader()
                        documents = loader.load(url)

                        # åˆ‡åˆ†æ–‡æ¡£
                        chunked_docs = []
                        for doc in documents:
                            chunks = split_text(doc.content)
                            for i, chunk in enumerate(chunks):
                                chunked_doc = Document(
                                    content=chunk,
                                    metadata={
                                        **doc.metadata,
                                        "chunk_index": i,
                                        "total_chunks": len(chunks),
                                    },
                                    source=doc.source,
                                )
                                chunked_docs.append(chunked_doc)

                        # å­˜å‚¨åˆ°å‘é‡åº“
                        vector_store.add_documents(chunked_docs)

                        st.success(f"âœ… æˆåŠŸæŠ“å–ï¼š{url}\nğŸ“Š å…± {len(chunked_docs)} ä¸ªæ–‡æ¡£å—")
                        st.session_state.documents_loaded = True
                        st.rerun()

                    except Exception as e:
                        st.error(f"âŒ æŠ“å–å¤±è´¥ï¼š{e}")


def render_file_management(vector_store: "VectorStore") -> None:
    """æ¸²æŸ“æ–‡ä»¶ç®¡ç†é¢æ¿

    Args:
        vector_store: å‘é‡å­˜å‚¨å®ä¾‹
    """
    with st.expander("ğŸ“ æ–‡ä»¶ç®¡ç†"):
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("åˆ·æ–°åˆ—è¡¨", use_container_width=True):
                st.rerun()

        with col2:
            files_count = len(vector_store.get_all_sources())
            st.metric("æ–‡ä»¶æ•°é‡", files_count)

        with col3:
            # æ¸…é™¤ä¸´æ—¶æ–‡ä»¶æŒ‰é’®
            if st.button("æ¸…é™¤æ—§æ•°æ®", use_container_width=True):
                all_sources = vector_store.get_all_sources()
                # åˆ é™¤æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼ˆä»¥ C:\ å¼€å¤´çš„ï¼‰
                deleted = 0
                for source in all_sources:
                    if source.startswith("C:\\") or source.startswith("/tmp/"):
                        vector_store.delete_by_source(source)
                        deleted += 1
                if deleted > 0:
                    st.success(f"âœ… å·²æ¸…é™¤ {deleted} ä¸ªæ—§æ–‡ä»¶")
                    st.rerun()
                else:
                    st.info("æ²¡æœ‰æ—§æ•°æ®éœ€è¦æ¸…é™¤")

        all_sources = vector_store.get_all_sources()

        # åˆ†ç»„æ˜¾ç¤ºï¼šä¸Šä¼ çš„æ–‡ä»¶å’Œç½‘é¡µ
        upload_files = [s for s in all_sources if s.startswith("upload:")]
        web_files = [s for s in all_sources if s.startswith("http")]
        old_files = [s for s in all_sources if s not in upload_files + web_files]

        if upload_files or web_files:
            # åˆ›å»ºæ˜¾ç¤ºåç§°æ˜ å°„
            source_to_display = {}
            for source in all_sources:
                if source.startswith("upload:"):
                    source_to_display[source] = source.replace("upload:", "")
                elif source.startswith("http"):
                    source_to_display[source] = source[:50] + "..." if len(source) > 50 else source
                else:
                    source_to_display[source] = Path(source).name

            display_names = [source_to_display[s] for s in all_sources]

            selected = st.multiselect(
                "é€‰æ‹©ç”¨äº RAG çš„æ–‡ä»¶",
                options=display_names,
                default=display_names,
                help="å–æ¶ˆé€‰æ‹©å¯ä» RAG ä¸­æ’é™¤"
            )

            # æ›´æ–°é€‰ä¸­çš„æ–‡ä»¶
            display_to_source = {v: k for k, v in source_to_display.items()}
            st.session_state.selected_sources = [
                display_to_source[name] for name in selected
            ]
        else:
            st.info("æš‚æ— æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£æˆ–æŠ“å–ç½‘é¡µ")
